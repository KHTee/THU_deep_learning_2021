{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-2: MLP for MNIST Classification\n",
    "\n",
    "### In this homework, you need to\n",
    "- #### implement SGD optimizer (`./optimizer.py`)\n",
    "- #### implement forward and backward for FCLayer (`layers/fc_layer.py`)\n",
    "- #### implement forward and backward for SigmoidLayer (`layers/sigmoid_layer.py`)\n",
    "- #### implement forward and backward for ReLULayer (`layers/relu_layer.py`)\n",
    "- #### implement EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
    "- #### implement SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "We use tensorflow tools to load dataset for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # Encode label with one-hot encoding\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyerparameters\n",
    "You can modify hyerparameters by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 20\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.001\n",
    "weight_decay = 0.1\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MLP with Euclidean Loss\n",
    "In part-1, you need to train a MLP with **Euclidean Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **./optimizer.py** and **criterion/euclidean_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import EuclideanLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = EuclideanLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 MLP with Euclidean Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/fc_layer.py** and **layers/sigmoid_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import FCLayer, SigmoidLayer\n",
    "\n",
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\Documents\\Lecture Note\\Deep learning\\hw\\homework-2\\homework2-mlp\\solver.py:15: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.3367\t Accuracy 0.1500\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.1276\t Accuracy 0.1806\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.1128\t Accuracy 0.2558\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.1058\t Accuracy 0.3241\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.1009\t Accuracy 0.3805\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.0974\t Accuracy 0.4264\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.0945\t Accuracy 0.4662\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.0924\t Accuracy 0.4968\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.0905\t Accuracy 0.5233\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.0890\t Accuracy 0.5456\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.0877\t Accuracy 0.5646\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0864\t Average training accuracy 0.5817\n",
      "Epoch [0]\t Average validation loss 0.0715\t Average validation accuracy 0.7990\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.0716\t Accuracy 0.8100\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.0732\t Accuracy 0.7727\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.0727\t Accuracy 0.7796\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.0728\t Accuracy 0.7751\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.0726\t Accuracy 0.7778\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.0724\t Accuracy 0.7794\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.0721\t Accuracy 0.7812\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.0720\t Accuracy 0.7823\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.0719\t Accuracy 0.7843\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.0717\t Accuracy 0.7865\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.0716\t Accuracy 0.7876\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0714\t Average training accuracy 0.7896\n",
      "Epoch [1]\t Average validation loss 0.0667\t Average validation accuracy 0.8536\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0667\t Accuracy 0.8700\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0686\t Accuracy 0.8245\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0685\t Accuracy 0.8253\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0689\t Accuracy 0.8174\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0688\t Accuracy 0.8177\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0687\t Accuracy 0.8180\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0687\t Accuracy 0.8179\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0687\t Accuracy 0.8175\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0687\t Accuracy 0.8181\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0686\t Accuracy 0.8187\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0686\t Accuracy 0.8187\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0685\t Average training accuracy 0.8197\n",
      "Epoch [2]\t Average validation loss 0.0647\t Average validation accuracy 0.8644\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0648\t Accuracy 0.8800\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0667\t Accuracy 0.8400\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0668\t Accuracy 0.8394\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0672\t Accuracy 0.8321\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0672\t Accuracy 0.8314\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0671\t Accuracy 0.8307\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0671\t Accuracy 0.8309\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0672\t Accuracy 0.8301\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0672\t Accuracy 0.8303\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0672\t Accuracy 0.8305\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0672\t Accuracy 0.8304\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0671\t Average training accuracy 0.8309\n",
      "Epoch [3]\t Average validation loss 0.0637\t Average validation accuracy 0.8736\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0637\t Accuracy 0.8800\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0657\t Accuracy 0.8455\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0658\t Accuracy 0.8446\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0662\t Accuracy 0.8375\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0662\t Accuracy 0.8365\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0662\t Accuracy 0.8365\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0662\t Accuracy 0.8370\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0663\t Accuracy 0.8360\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0663\t Accuracy 0.8364\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0663\t Accuracy 0.8365\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0663\t Accuracy 0.8361\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0663\t Average training accuracy 0.8365\n",
      "Epoch [4]\t Average validation loss 0.0630\t Average validation accuracy 0.8788\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0631\t Accuracy 0.8900\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0651\t Accuracy 0.8496\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0652\t Accuracy 0.8484\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0656\t Accuracy 0.8411\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0656\t Accuracy 0.8405\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0656\t Accuracy 0.8405\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0656\t Accuracy 0.8408\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0658\t Accuracy 0.8397\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0658\t Accuracy 0.8402\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0658\t Accuracy 0.8402\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0658\t Accuracy 0.8394\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0658\t Average training accuracy 0.8399\n",
      "Epoch [5]\t Average validation loss 0.0626\t Average validation accuracy 0.8796\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0627\t Accuracy 0.8900\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0646\t Accuracy 0.8518\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0648\t Accuracy 0.8502\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0652\t Accuracy 0.8437\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0653\t Accuracy 0.8430\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0653\t Accuracy 0.8430\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0653\t Accuracy 0.8437\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0654\t Accuracy 0.8426\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0654\t Accuracy 0.8433\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0654\t Accuracy 0.8433\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0655\t Accuracy 0.8426\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0654\t Average training accuracy 0.8429\n",
      "Epoch [6]\t Average validation loss 0.0624\t Average validation accuracy 0.8818\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0624\t Accuracy 0.8900\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0643\t Accuracy 0.8531\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0645\t Accuracy 0.8523\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0650\t Accuracy 0.8454\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0650\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0650\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0650\t Accuracy 0.8455\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0651\t Accuracy 0.8444\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0652\t Accuracy 0.8452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0652\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0652\t Accuracy 0.8444\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0652\t Average training accuracy 0.8449\n",
      "Epoch [7]\t Average validation loss 0.0622\t Average validation accuracy 0.8834\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0622\t Accuracy 0.8900\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0641\t Accuracy 0.8547\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0643\t Accuracy 0.8536\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0648\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0648\t Accuracy 0.8456\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0648\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0648\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0650\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0650\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0650\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0651\t Accuracy 0.8456\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0650\t Average training accuracy 0.8461\n",
      "Epoch [8]\t Average validation loss 0.0621\t Average validation accuracy 0.8838\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0621\t Accuracy 0.8900\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0640\t Accuracy 0.8563\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0641\t Accuracy 0.8548\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0647\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0647\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0647\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0647\t Accuracy 0.8476\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0648\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0649\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0649\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0650\t Accuracy 0.8466\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0649\t Average training accuracy 0.8470\n",
      "Epoch [9]\t Average validation loss 0.0620\t Average validation accuracy 0.8852\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0620\t Accuracy 0.9000\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0639\t Accuracy 0.8575\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0641\t Accuracy 0.8553\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0646\t Accuracy 0.8475\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0646\t Accuracy 0.8475\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0646\t Accuracy 0.8480\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0646\t Accuracy 0.8485\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0648\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0648\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0648\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0649\t Accuracy 0.8474\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0648\t Average training accuracy 0.8478\n",
      "Epoch [10]\t Average validation loss 0.0619\t Average validation accuracy 0.8858\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0619\t Accuracy 0.9000\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0638\t Accuracy 0.8575\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0640\t Accuracy 0.8556\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0645\t Accuracy 0.8479\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0645\t Accuracy 0.8478\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0645\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0646\t Accuracy 0.8487\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0647\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0647\t Accuracy 0.8484\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0648\t Accuracy 0.8484\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0648\t Accuracy 0.8480\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0648\t Average training accuracy 0.8484\n",
      "Epoch [11]\t Average validation loss 0.0619\t Average validation accuracy 0.8860\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0619\t Accuracy 0.9000\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0638\t Accuracy 0.8580\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0639\t Accuracy 0.8557\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0645\t Accuracy 0.8481\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0645\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0645\t Accuracy 0.8484\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0645\t Accuracy 0.8488\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0647\t Accuracy 0.8478\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0647\t Accuracy 0.8486\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0647\t Accuracy 0.8486\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0648\t Accuracy 0.8482\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0647\t Average training accuracy 0.8485\n",
      "Epoch [12]\t Average validation loss 0.0619\t Average validation accuracy 0.8858\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0618\t Accuracy 0.9000\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0638\t Accuracy 0.8576\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0639\t Accuracy 0.8555\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0644\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0645\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0645\t Accuracy 0.8481\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0645\t Accuracy 0.8486\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0646\t Accuracy 0.8475\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0647\t Accuracy 0.8484\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0647\t Accuracy 0.8485\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0648\t Accuracy 0.8481\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0647\t Average training accuracy 0.8485\n",
      "Epoch [13]\t Average validation loss 0.0618\t Average validation accuracy 0.8860\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0618\t Accuracy 0.9000\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0637\t Accuracy 0.8573\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0639\t Accuracy 0.8553\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0644\t Accuracy 0.8480\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0644\t Accuracy 0.8477\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0645\t Accuracy 0.8485\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0645\t Accuracy 0.8489\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0646\t Accuracy 0.8477\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0646\t Accuracy 0.8486\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0647\t Accuracy 0.8488\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0647\t Accuracy 0.8484\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0647\t Average training accuracy 0.8487\n",
      "Epoch [14]\t Average validation loss 0.0618\t Average validation accuracy 0.8856\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0618\t Accuracy 0.9000\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0637\t Accuracy 0.8573\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0639\t Accuracy 0.8554\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0644\t Accuracy 0.8481\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0644\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0644\t Accuracy 0.8486\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0645\t Accuracy 0.8490\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0646\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0646\t Accuracy 0.8489\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0647\t Accuracy 0.8491\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0647\t Accuracy 0.8487\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0647\t Average training accuracy 0.8489\n",
      "Epoch [15]\t Average validation loss 0.0618\t Average validation accuracy 0.8852\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0618\t Accuracy 0.9000\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0637\t Accuracy 0.8571\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0639\t Accuracy 0.8552\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0644\t Accuracy 0.8480\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0644\t Accuracy 0.8479\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0644\t Accuracy 0.8485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0645\t Accuracy 0.8489\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0646\t Accuracy 0.8478\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0646\t Accuracy 0.8487\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0647\t Accuracy 0.8489\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0647\t Accuracy 0.8487\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0647\t Average training accuracy 0.8489\n",
      "Epoch [16]\t Average validation loss 0.0618\t Average validation accuracy 0.8848\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0618\t Accuracy 0.9000\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0637\t Accuracy 0.8565\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0639\t Accuracy 0.8550\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0644\t Accuracy 0.8477\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0644\t Accuracy 0.8480\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0644\t Accuracy 0.8485\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0644\t Accuracy 0.8489\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0646\t Accuracy 0.8477\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0646\t Accuracy 0.8486\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0647\t Accuracy 0.8488\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0647\t Accuracy 0.8486\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0647\t Average training accuracy 0.8488\n",
      "Epoch [17]\t Average validation loss 0.0618\t Average validation accuracy 0.8850\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0618\t Accuracy 0.9000\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0637\t Accuracy 0.8567\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0639\t Accuracy 0.8547\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0644\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0644\t Accuracy 0.8481\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0644\t Accuracy 0.8486\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0644\t Accuracy 0.8490\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0646\t Accuracy 0.8478\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0646\t Accuracy 0.8486\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0647\t Accuracy 0.8489\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0647\t Accuracy 0.8486\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0647\t Average training accuracy 0.8488\n",
      "Epoch [18]\t Average validation loss 0.0618\t Average validation accuracy 0.8850\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0618\t Accuracy 0.9000\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0637\t Accuracy 0.8569\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0639\t Accuracy 0.8549\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0644\t Accuracy 0.8482\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0644\t Accuracy 0.8486\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0644\t Accuracy 0.8489\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0644\t Accuracy 0.8494\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0646\t Accuracy 0.8482\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0646\t Accuracy 0.8490\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0647\t Accuracy 0.8492\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0647\t Accuracy 0.8490\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0646\t Average training accuracy 0.8491\n",
      "Epoch [19]\t Average validation loss 0.0618\t Average validation accuracy 0.8852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.8601.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MLP with Euclidean Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/relu_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import ReLULayer\n",
    "\n",
    "reluMLP = Network()\n",
    "# TODO build ReLUMLP with FCLayer and ReLULayer\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.2484\t Accuracy 0.0400\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.1690\t Accuracy 0.1116\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.1469\t Accuracy 0.2217\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.1347\t Accuracy 0.2977\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.1261\t Accuracy 0.3614\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.1194\t Accuracy 0.4138\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.1143\t Accuracy 0.4552\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.1103\t Accuracy 0.4875\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.1069\t Accuracy 0.5154\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.1041\t Accuracy 0.5393\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.1016\t Accuracy 0.5594\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0993\t Average training accuracy 0.5786\n",
      "Epoch [0]\t Average validation loss 0.0734\t Average validation accuracy 0.8058\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.0736\t Accuracy 0.8000\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.0744\t Accuracy 0.7869\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.0736\t Accuracy 0.7935\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.0734\t Accuracy 0.7911\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.0728\t Accuracy 0.7948\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.0721\t Accuracy 0.8012\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.0715\t Accuracy 0.8040\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.0711\t Accuracy 0.8072\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.0706\t Accuracy 0.8107\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.0702\t Accuracy 0.8138\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.0698\t Accuracy 0.8160\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0693\t Average training accuracy 0.8198\n",
      "Epoch [1]\t Average validation loss 0.0616\t Average validation accuracy 0.8802\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0612\t Accuracy 0.8700\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0632\t Accuracy 0.8598\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0629\t Accuracy 0.8587\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0631\t Accuracy 0.8558\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0628\t Accuracy 0.8572\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0625\t Accuracy 0.8601\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0622\t Accuracy 0.8612\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0620\t Accuracy 0.8624\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0618\t Accuracy 0.8639\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0615\t Accuracy 0.8651\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0614\t Accuracy 0.8655\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0610\t Average training accuracy 0.8674\n",
      "Epoch [2]\t Average validation loss 0.0552\t Average validation accuracy 0.9060\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0550\t Accuracy 0.9000\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0571\t Accuracy 0.8861\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0570\t Accuracy 0.8850\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0573\t Accuracy 0.8809\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0571\t Accuracy 0.8814\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0569\t Accuracy 0.8833\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0567\t Accuracy 0.8840\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0567\t Accuracy 0.8850\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0565\t Accuracy 0.8860\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0564\t Accuracy 0.8865\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0563\t Accuracy 0.8866\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0561\t Average training accuracy 0.8878\n",
      "Epoch [3]\t Average validation loss 0.0510\t Average validation accuracy 0.9228\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0509\t Accuracy 0.9000\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0529\t Accuracy 0.9033\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0530\t Accuracy 0.9024\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0534\t Accuracy 0.8979\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0533\t Accuracy 0.8983\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0531\t Accuracy 0.8992\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0530\t Accuracy 0.8994\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0530\t Accuracy 0.8997\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0529\t Accuracy 0.9004\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0528\t Accuracy 0.9007\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0528\t Accuracy 0.9006\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0526\t Average training accuracy 0.9012\n",
      "Epoch [4]\t Average validation loss 0.0480\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0478\t Accuracy 0.9200\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0499\t Accuracy 0.9131\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0500\t Accuracy 0.9117\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0505\t Accuracy 0.9075\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0504\t Accuracy 0.9079\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0503\t Accuracy 0.9087\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0503\t Accuracy 0.9086\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0503\t Accuracy 0.9091\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0502\t Accuracy 0.9096\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0502\t Accuracy 0.9098\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0502\t Accuracy 0.9094\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0500\t Average training accuracy 0.9098\n",
      "Epoch [5]\t Average validation loss 0.0457\t Average validation accuracy 0.9364\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0454\t Accuracy 0.9300\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0476\t Accuracy 0.9216\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0478\t Accuracy 0.9196\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0484\t Accuracy 0.9162\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0483\t Accuracy 0.9160\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0482\t Accuracy 0.9165\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0482\t Accuracy 0.9165\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0482\t Accuracy 0.9164\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0482\t Accuracy 0.9167\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0481\t Accuracy 0.9170\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0482\t Accuracy 0.9164\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0480\t Average training accuracy 0.9166\n",
      "Epoch [6]\t Average validation loss 0.0440\t Average validation accuracy 0.9402\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0435\t Accuracy 0.9300\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0459\t Accuracy 0.9275\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0462\t Accuracy 0.9242\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0467\t Accuracy 0.9214\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0466\t Accuracy 0.9210\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0465\t Accuracy 0.9212\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0466\t Accuracy 0.9213\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0466\t Accuracy 0.9209\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0466\t Accuracy 0.9211\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0466\t Accuracy 0.9214\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0466\t Accuracy 0.9209\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0465\t Average training accuracy 0.9210\n",
      "Epoch [7]\t Average validation loss 0.0426\t Average validation accuracy 0.9442\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0420\t Accuracy 0.9400\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0445\t Accuracy 0.9302\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0448\t Accuracy 0.9271\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0454\t Accuracy 0.9250\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0453\t Accuracy 0.9250\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0452\t Accuracy 0.9252\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0453\t Accuracy 0.9254\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0453\t Accuracy 0.9250\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0453\t Accuracy 0.9252\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0453\t Accuracy 0.9256\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0454\t Accuracy 0.9249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.0453\t Average training accuracy 0.9249\n",
      "Epoch [8]\t Average validation loss 0.0415\t Average validation accuracy 0.9474\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0408\t Accuracy 0.9400\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0434\t Accuracy 0.9339\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0438\t Accuracy 0.9309\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0443\t Accuracy 0.9289\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0442\t Accuracy 0.9295\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0442\t Accuracy 0.9296\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0442\t Accuracy 0.9295\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0443\t Accuracy 0.9290\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0443\t Accuracy 0.9289\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0443\t Accuracy 0.9292\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0444\t Accuracy 0.9284\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0443\t Average training accuracy 0.9283\n",
      "Epoch [9]\t Average validation loss 0.0406\t Average validation accuracy 0.9498\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0398\t Accuracy 0.9500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0425\t Accuracy 0.9378\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0429\t Accuracy 0.9340\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0434\t Accuracy 0.9318\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0433\t Accuracy 0.9320\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0433\t Accuracy 0.9322\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0433\t Accuracy 0.9323\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0434\t Accuracy 0.9317\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0434\t Accuracy 0.9315\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0434\t Accuracy 0.9316\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0435\t Accuracy 0.9308\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0434\t Average training accuracy 0.9308\n",
      "Epoch [10]\t Average validation loss 0.0398\t Average validation accuracy 0.9526\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0390\t Accuracy 0.9500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0417\t Accuracy 0.9390\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0421\t Accuracy 0.9352\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0426\t Accuracy 0.9338\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0425\t Accuracy 0.9343\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0425\t Accuracy 0.9344\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0426\t Accuracy 0.9346\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0426\t Accuracy 0.9343\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0427\t Accuracy 0.9341\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0427\t Accuracy 0.9342\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0428\t Accuracy 0.9334\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0427\t Average training accuracy 0.9333\n",
      "Epoch [11]\t Average validation loss 0.0391\t Average validation accuracy 0.9560\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0383\t Accuracy 0.9500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0410\t Accuracy 0.9416\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0414\t Accuracy 0.9373\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0419\t Accuracy 0.9359\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0419\t Accuracy 0.9365\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0418\t Accuracy 0.9367\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0419\t Accuracy 0.9368\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0420\t Accuracy 0.9365\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0420\t Accuracy 0.9363\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0420\t Accuracy 0.9363\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0421\t Accuracy 0.9354\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0420\t Average training accuracy 0.9353\n",
      "Epoch [12]\t Average validation loss 0.0385\t Average validation accuracy 0.9570\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0378\t Accuracy 0.9500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0404\t Accuracy 0.9439\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0409\t Accuracy 0.9389\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0414\t Accuracy 0.9377\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0413\t Accuracy 0.9386\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0412\t Accuracy 0.9388\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0413\t Accuracy 0.9391\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0414\t Accuracy 0.9387\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0414\t Accuracy 0.9384\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0415\t Accuracy 0.9384\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0415\t Accuracy 0.9374\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0415\t Average training accuracy 0.9373\n",
      "Epoch [13]\t Average validation loss 0.0380\t Average validation accuracy 0.9598\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0373\t Accuracy 0.9500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0399\t Accuracy 0.9455\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0403\t Accuracy 0.9410\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0408\t Accuracy 0.9397\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0408\t Accuracy 0.9407\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0407\t Accuracy 0.9407\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0408\t Accuracy 0.9408\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0409\t Accuracy 0.9403\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0409\t Accuracy 0.9398\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0409\t Accuracy 0.9397\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0410\t Accuracy 0.9388\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0410\t Average training accuracy 0.9387\n",
      "Epoch [14]\t Average validation loss 0.0376\t Average validation accuracy 0.9608\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0369\t Accuracy 0.9500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0394\t Accuracy 0.9476\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0399\t Accuracy 0.9429\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0404\t Accuracy 0.9413\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0403\t Accuracy 0.9423\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0402\t Accuracy 0.9421\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0403\t Accuracy 0.9421\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0404\t Accuracy 0.9418\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0405\t Accuracy 0.9414\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0405\t Accuracy 0.9412\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0406\t Accuracy 0.9403\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0405\t Average training accuracy 0.9402\n",
      "Epoch [15]\t Average validation loss 0.0372\t Average validation accuracy 0.9610\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0366\t Accuracy 0.9600\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0390\t Accuracy 0.9490\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0395\t Accuracy 0.9443\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0399\t Accuracy 0.9426\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0399\t Accuracy 0.9436\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0398\t Accuracy 0.9433\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0399\t Accuracy 0.9431\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0400\t Accuracy 0.9429\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0400\t Accuracy 0.9425\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0401\t Accuracy 0.9423\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0402\t Accuracy 0.9414\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0401\t Average training accuracy 0.9413\n",
      "Epoch [16]\t Average validation loss 0.0368\t Average validation accuracy 0.9616\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0363\t Accuracy 0.9600\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0386\t Accuracy 0.9502\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0391\t Accuracy 0.9448\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0396\t Accuracy 0.9435\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0395\t Accuracy 0.9446\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0394\t Accuracy 0.9443\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0395\t Accuracy 0.9440\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0396\t Accuracy 0.9437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0397\t Accuracy 0.9433\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0397\t Accuracy 0.9431\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0398\t Accuracy 0.9423\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0397\t Average training accuracy 0.9422\n",
      "Epoch [17]\t Average validation loss 0.0365\t Average validation accuracy 0.9616\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0360\t Accuracy 0.9600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0382\t Accuracy 0.9506\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0387\t Accuracy 0.9455\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0392\t Accuracy 0.9445\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0391\t Accuracy 0.9455\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0391\t Accuracy 0.9453\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0392\t Accuracy 0.9450\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0393\t Accuracy 0.9448\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0393\t Accuracy 0.9445\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0394\t Accuracy 0.9443\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0395\t Accuracy 0.9434\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0394\t Average training accuracy 0.9434\n",
      "Epoch [18]\t Average validation loss 0.0362\t Average validation accuracy 0.9624\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0357\t Accuracy 0.9600\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0379\t Accuracy 0.9510\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0384\t Accuracy 0.9462\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0389\t Accuracy 0.9456\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0388\t Accuracy 0.9464\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0388\t Accuracy 0.9461\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0389\t Accuracy 0.9458\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0390\t Accuracy 0.9456\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0390\t Accuracy 0.9453\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0390\t Accuracy 0.9452\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0391\t Accuracy 0.9444\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0391\t Average training accuracy 0.9444\n",
      "Epoch [19]\t Average validation loss 0.0359\t Average validation accuracy 0.9632\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9495.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLP with Softmax Cross-Entropy Loss\n",
    "In part-2, you need to train a MLP with **Softmax Cross-Entropy Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively again.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **criterion/softmax_cross_entropy_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MLP with Softmax Cross-Entropy Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "bad allocation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-e0536a27d21f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msigmoidMLP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigmoid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigmoid_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigmoidMLP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisp_freq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Lecture Note\\Deep learning\\hw\\homework-2\\homework2-mlp\\solver.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, dataset, max_epoch, batch_size, disp_freq)\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mtrain_get_next\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[0mtmp3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_next\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m                 \u001b[0mtmp3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0mtmp4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\logit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    958\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\logit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1181\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1182\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\logit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1358\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1359\u001b[0m                            run_metadata)\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\logit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\logit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1346\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m   1350\u001b[0m                                       target_list, run_metadata)\n",
      "\u001b[1;32m~\\.conda\\envs\\logit\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1386\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1388\u001b[1;33m       \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1390\u001b[0m   \u001b[1;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: bad allocation"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 MLP with Softmax Cross-Entropy Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "# Build ReLUMLP with FCLayer and ReLULayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~You have finished homework2-mlp, congratulations!~~  \n",
    "\n",
    "**Next, according to the requirements 4) of report:**\n",
    "### **You need to construct a two-hidden-layer MLP, using any activation function and loss function.**\n",
    "\n",
    "**Note: Please insert some new cells blow (using '+' bottom in the toolbar) refer to above codes. Do not modify the former code directly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 2-layer MLP with Softmax Cross-Entropy Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer, EuclideanLossLayer\n",
    "from layers import FCLayer, SigmoidLayer, ReLULayer\n",
    "from optimizer import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 60\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 3e-3\n",
    "weight_decay = 1e-3\n",
    "\n",
    "disp_freq = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = EuclideanLossLayer()\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)\n",
    "\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 32))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(32, 10))\n",
    "\n",
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 32))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(32, 10))\n",
    "\n",
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
